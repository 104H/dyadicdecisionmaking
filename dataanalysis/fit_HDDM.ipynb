{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5763ef59",
   "metadata": {},
   "source": [
    "# preferably run this script on the grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958a5f9f",
   "metadata": {},
   "source": [
    "Based on Anne Urai's script fitHDDM.py\n",
    "\n",
    "\n",
    "Changes:\n",
    "1. Parameter estimations in dyadic case\n",
    "2. Flexible handling of no. of lagged/previous trials tested for history effect\n",
    "3. Encoding specific to our experiment\n",
    "4. Simplified dataset handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "910c492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import hddm\n",
    "import kabuki\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "489666fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recode_4stimcoding(mydata):\n",
    "    #code stimulus and response direction left as 0, leave direction right as 1.\n",
    "    \n",
    "    mydata.loc[mydata['direction']==-1,'direction'] = 0\n",
    "    mydata.loc[mydata['response']==-1,'response'] = 0\n",
    "    for col in mydata.columns.tolist():\n",
    "        if ('stim' in col) or ('resp' in col):\n",
    "            mydata.loc[mydata[col]==-1,col] = 0\n",
    "    \n",
    "    return mydata\n",
    "\n",
    "def z_link_func(x):\n",
    "    return 1 / (1 + np.exp(-(x.values.ravel())))\n",
    "\n",
    "def aic(self):\n",
    "    k = len(self.get_stochastics())\n",
    "    logp = sum([x.logp for x in self.get_observeds()['node']])\n",
    "    return 2 * k - 2 * logp\n",
    "\n",
    "\n",
    "def bic(self):\n",
    "    k = len(self.get_stochastics())\n",
    "    n = len(self.data)\n",
    "    logp = sum([x.logp for x in self.get_observeds()['node']])\n",
    "    return -2 * logp + k * np.log(n)\n",
    "\n",
    "def concat_models(mypath, model_name, nchains=30):\n",
    "    traces = range(1,nchains+1)\n",
    "\n",
    "    # CHECK IF COMBINED MODEL EXISTS\n",
    "    if os.path.isfile(os.path.join(mypath, model_name, 'modelfit-combined.model')):\n",
    "        print(\"Combined Model exists: {}\".format(os.path.join(mypath, model_name, 'modelfit-combined.model')))\n",
    "    else:\n",
    "        # ============================================ #\n",
    "        # APPEND MODELS\n",
    "        # ============================================ #\n",
    "        allmodels = []\n",
    "        print(\"Combining all traces for %s\" % model_name)\n",
    "        for trace_id in traces:  # how many chains were run?\n",
    "            model_filename = os.path.join(mypath, model_name, 'modelfit-md%d.model' % trace_id)\n",
    "            if os.path.isfile(model_filename) == True:  # if not, this model has to be rerun\n",
    "                print(model_filename)\n",
    "                thism = hddm.load(model_filename)\n",
    "                allmodels.append(thism)  # now append into a list\n",
    "            else:\n",
    "                print(\"Not found: trace_id {:2d}\".format(trace_id))\n",
    "                \n",
    "        if len(allmodels) != nchains:\n",
    "            return None\n",
    "        # ============================================ #\n",
    "        # CHECK CONVERGENCE if all traces were found\n",
    "        # ============================================ #    \n",
    "        print(\"Performing gelman rubin convergence test\\n\")\n",
    "        try:\n",
    "            gr = hddm.analyze.gelman_rubin(allmodels)\n",
    "            # save\n",
    "            text_file = open(os.path.join(mypath, model_name, 'gelman_rubin.txt'), 'w')\n",
    "            for p in gr.items():\n",
    "                text_file.write(\"%s,%s\\n\" % p)\n",
    "                # print a warning when non-convergence is detected\n",
    "                # Values should be close to 1 and not larger than 1.02 which would indicate convergence problems.\n",
    "                # https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3731670/\n",
    "                if abs(p[1] - 1) > 0.02:\n",
    "                    print(\"non-convergence found, %s:%s\" % p)\n",
    "            text_file.close()\n",
    "            print(\"written gelman rubin stats to file\")\n",
    "        except Exception as e:\n",
    "            print(\"Error: {}\".format(e))\n",
    "        m = kabuki.utils.concat_models(allmodels) #creates one model from all chains\n",
    "\n",
    "        # ============================================ #\n",
    "        # SAVE THE FULL MODEL\n",
    "        # ============================================ #\n",
    "\n",
    "        m.save(os.path.join(mypath, model_name, 'modelfit-combined.model'))  # save combined modelto disk\n",
    "        print(\"Concatenated model saved!\")\n",
    "        \n",
    "        # ============================================ #\n",
    "        # SAVE POINT ESTIMATES\n",
    "        # ============================================ #\n",
    "\n",
    "        print(\"saving stats...\")\n",
    "        results = m.gen_stats()  # point estimate for each parameter and subject\n",
    "        results.to_csv(os.path.join(mypath, model_name, 'results-combined.csv'))\n",
    "\n",
    "        # save the DIC for this model\n",
    "        text_file = open(os.path.join(mypath, model_name, 'DIC-combined.txt'), 'w')\n",
    "        text_file.write(\"Combined model: {}\\n\".format(m.dic))\n",
    "        text_file.close()\n",
    "        print('done')\n",
    "        \n",
    "        # ============================================ #\n",
    "        # SAVE TRACES\n",
    "        # ============================================ #\n",
    "\n",
    "        print(\"saving traces...\")\n",
    "        # get the names for all nodes that are available here\n",
    "        group_traces = m.get_group_traces()\n",
    "        group_traces.to_csv(os.path.join(mypath, model_name, 'group_traces.csv'))\n",
    "\n",
    "        all_traces = m.get_traces()\n",
    "        all_traces.to_csv(os.path.join(mypath, model_name, 'all_traces.csv'))\n",
    "        print('done')\n",
    "        \n",
    "        # ============================================ #\n",
    "        # CONCATENATE MODEL COMPARISON\n",
    "        # ============================================ #\n",
    "        # average model comparison values across chains\n",
    "        print('concatenating model comparison...')\n",
    "        fls = glob.glob(os.path.join(mypath, model_name, 'model_comparison_md*.csv'))\n",
    "        tmpdf = pd.concat([pd.read_csv(f) for f in fls ])\n",
    "        # average over chains\n",
    "        df2 = tmpdf.mean()\n",
    "        df2 = tmpdf.describe().loc[['mean']]\n",
    "        df2.to_csv(os.path.join(mypath, model_name, 'model_comparison.csv')) # save comparison to disk\n",
    "        print('done')\n",
    "\n",
    "\n",
    "        # DELETE FILES to save space\n",
    "        print(\"Now deleting files for seperate chains...\")\n",
    "        for fl in glob.glob(os.path.join(mypath, model_name, 'modelfit-md*.model')):\n",
    "            print(fl)\n",
    "            os.remove(fl)\n",
    "        for fl in glob.glob(os.path.join(mypath, model_name, 'modelfit-md*.db')):\n",
    "            if not '-md1.db' in fl: #needed here to load the pickled db in PPC\n",
    "                print(fl)\n",
    "                os.remove(fl)\n",
    "        for fl in glob.glob(os.path.join(mypath, model_name, 'model_comparison_md*.csv')):\n",
    "            print(fl)\n",
    "            os.remove(fl)\n",
    "        for fl in glob.glob(os.path.join(mypath, model_name, 'DIC-md*.txt')):\n",
    "            print(fl)\n",
    "            os.remove(fl)\n",
    "        print('DONE!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7b2211db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(mydata, model_name, trace_id, nlag=0):\n",
    "    \n",
    "    #checks before model is created\n",
    "    if \"nohist\" not in model_name and nlag == 0:\n",
    "        print(\"For all models with history effects, 'nlag' must be non-zero\")\n",
    "        exit(0)\n",
    "    elif \"nohist\" in model_name and nlag != 0:\n",
    "        print(\"'nlag' specified but model is without history effect. 'nlag' value is ignored\\n\")\n",
    "        nlag = 0\n",
    "         \n",
    "    if 'regress' in model_name:\n",
    "        if nlag != 0:\n",
    "            lags = range(1,nlag+1)\n",
    "            resp_cols = ['l' + str(i) + '_resp' for i in lags]\n",
    "            stim_cols = ['l' + str(i) + '_stim' for i in lags]\n",
    "            resps = \" + \".join(resp_cols)\n",
    "            respstim = \" + \".join(resp_cols + stim_cols)\n",
    "            for col in stim_cols:\n",
    "                mydata = mydata[mydata[col].notna()]\n",
    "    else:\n",
    "        mydata = recode_4stimcoding(mydata)\n",
    "        if nlag != 0:\n",
    "            col_resp = 'l' + str(nlag) + '_resp'\n",
    "            col_stim = 'l' + str(nlag) + '_stim'\n",
    "            col_stim = 'l' + str(nlag) + '_subject'\n",
    "            col_repeat = 'l' + str(nlag) + '_repeat'\n",
    "             \n",
    "    if model_name == 'stimcoding_nohist': # NO HISTORY FOR MODEL COMPARISON\n",
    "        m = hddm.HDDMStimCoding(mydata, stim_col='direction', split_param='v',\n",
    "                drift_criterion=True, bias=True, p_outlier=0.05,\n",
    "                include=('sv', 'sz'), group_only_nodes=['sv', 'sz'])\n",
    "    elif model_name == 'stimcoding_nohist_svgroup': # DOES DRIFT RATE VARIABILITY REDUCE? GROUP\n",
    "        # add a group to indicate bias magnitude\n",
    "        sjrepetition = mydata.groupby(['subj_idx'])['l1_repeat'].mean().reset_index()\n",
    "        sjrepetition['biasgroup'] = pd.qcut(np.abs(sjrepetition['l1_repeat'] - 0.5), 3, labels=False)\n",
    "        mydata2 = pd.merge(mydata, sjrepetition, on='subj_idx', how='inner')\n",
    "        m = hddm.HDDMStimCoding(mydata2, stim_col='direction', split_param='v',\n",
    "                drift_criterion=True, bias=True, p_outlier=0.05,\n",
    "                include=('sv', 'sz'), group_only_nodes=['sz'],\n",
    "                depends_on={'sv': ['biasgroup']})\n",
    "    elif model_name == 'stimcoding_dc_resp':\n",
    "        m = hddm.HDDMStimCoding(mydata, stim_col='direction', split_param='v',\n",
    "                drift_criterion=True, bias=True, p_outlier=0.05,\n",
    "                include=('sv', 'sz'), group_only_nodes=['sv', 'sz'],\n",
    "                depends_on={'dc':[col_resp]})\n",
    "    elif model_name == 'stimcoding_z_resp':\n",
    "        m = hddm.HDDMStimCoding(mydata, stim_col='direction', split_param='v',\n",
    "                drift_criterion=True, bias=True, p_outlier=0.05,\n",
    "                include=('sv', 'sz'), group_only_nodes=['sv', 'sz'],\n",
    "                depends_on={'z':[col_resp]})\n",
    "    elif model_name == 'stimcoding_dc_z_resp':\n",
    "        m = hddm.HDDMStimCoding(mydata, stim_col='direction', split_param='v',\n",
    "                drift_criterion=True, bias=True, p_outlier=0.05,\n",
    "                include=('sv', 'sz'), group_only_nodes=['sv', 'sz'],\n",
    "                depends_on={'dc':[col_resp], 'z':[col_resp]})\n",
    "    elif model_name == 'stimcoding_dc_stim': # STIMCODING PREVSTIM\n",
    "        m = hddm.HDDMStimCoding(mydata, stim_col='direction', split_param='v',\n",
    "                drift_criterion=True, bias=True, p_outlier=0.05,\n",
    "                include=('sv', 'sz'), group_only_nodes=['sv', 'sz'],\n",
    "                depends_on={'dc':[col_stim]})\n",
    "    elif model_name == 'stimcoding_z_stim':\n",
    "        m = hddm.HDDMStimCoding(mydata, stim_col='direction', split_param='v',\n",
    "                drift_criterion=True, bias=True, p_outlier=0.05,\n",
    "                include=('sv', 'sz'), group_only_nodes=['sv', 'sz'],\n",
    "                depends_on={'z':[col_stim]})\n",
    "    elif model_name == 'stimcoding_dc_z_stim':\n",
    "        m = hddm.HDDMStimCoding(mydata, stim_col='direction', split_param='v',\n",
    "                drift_criterion=True, bias=True, p_outlier=0.05,\n",
    "                include=('sv', 'sz'), group_only_nodes=['sv', 'sz'],\n",
    "                depends_on={'dc':[col_stim], 'z':[col_stim]})\n",
    "    elif model_name == 'stimcoding_dc_z_st_resp': # also estimate across-trial variability in nondecision time\n",
    "        m = hddm.HDDMStimCoding(mydata, stim_col='direction', split_param='v',\n",
    "                drift_criterion=True, bias=True, p_outlier=0.05,\n",
    "                include=('sv', 'sz', 'st'), group_only_nodes=['sv', 'sz', 'st'],\n",
    "                depends_on={'dc':[col_resp], 'z':[col_resp]})\n",
    "    elif model_name == 'stimcoding_dc_z_resp_svgroup':\n",
    "        # add a group to indicate bias magnitude\n",
    "        sjrepetition = mydata.groupby(['subj_idx'])[col_repeat].mean().reset_index()\n",
    "        sjrepetition['biasgroup'] = pd.qcut(np.abs(sjrepetition[col_repeat] - 0.5), 3, labels=False)\n",
    "        mydata2 = pd.merge(mydata, sjrepetition, on='subj_idx', how='inner')\n",
    "        m = hddm.HDDMStimCoding(mydata2, stim_col='direction', split_param='v',\n",
    "                drift_criterion=True, bias=True, p_outlier=0.05,\n",
    "                include=('sv', 'sz'), group_only_nodes=['sz'],\n",
    "                depends_on={'dc':[col_resp], 'z':[col_resp], 'sv': ['biasgroup']})\n",
    "    elif model_name == 'stimcoding_dc_z_resp_groupsplit':# SEPARATE FIT FOR REPEATERS AND ALTERNATORS\n",
    "        # add coding for repeaters and alternators\n",
    "        sjrepetition = mydata.groupby(['subj_idx'])[col_repeat].mean().reset_index()\n",
    "        sjrepetition['group'] = np.sign(sjrepetition[col_repeat] - 0.5)\n",
    "        mydata2 = pd.merge(mydata, sjrepetition, on='subj_idx', how='inner')\n",
    "        m = hddm.HDDMStimCoding(mydata2, stim_col='direction', split_param='v',\n",
    "                drift_criterion=True, bias=True, p_outlier=0.05,\n",
    "                include=('sv', 'sz'), group_only_nodes=['sv', 'sz'],\n",
    "                depends_on={'dc':[col_resp, 'group'], 'z':[col_resp, 'group']})\n",
    "    elif model_name == 'stimcoding_dc_z_resp_congruency': #SPLIT BY CONGRUENCE BETWEEN PREVIOUS CHOICE AND CURRENT STIMULUS\n",
    "        # compute a double (not boolean) to indicate congruence\n",
    "        mydata['congruent'] = (mydata[col_resp] == mydata['direction']) * 1\n",
    "        m = hddm.HDDMStimCoding(mydata, stim_col='direction', split_param='v',\n",
    "                drift_criterion=True, bias=True, p_outlier=0.05,\n",
    "                include=('sv', 'sz'), group_only_nodes=['sv', 'sz'],\n",
    "                depends_on={'dc':[col_resp, 'congruent'], 'z':[col_resp, 'congruent']})\n",
    "    # ============================================ #\n",
    "    # REGRESSION MODELS WITH MULTIPLE LAGS\n",
    "    # ============================================ #\n",
    "    elif model_name == 'regress_nohist':\n",
    "        # only stimulus dependence\n",
    "        v_reg = {'model': 'v ~ 1 + direction', 'link_func': lambda x:x}\n",
    "        # specify that we want individual parameters for all regressors, see email Gilles 22.02.2017\n",
    "        m = hddm.HDDMRegressor(mydata, v_reg,\n",
    "            include=['z', 'sv'], group_only_nodes=['sv'],\n",
    "            group_only_regressors=False, keep_regressor_trace=False, p_outlier=0.05)\n",
    "    elif model_name == 'regress_dc_resp': #only previous response\n",
    "        v_reg = {'model': 'v ~ 1 + direction + ' + resps, 'link_func': lambda x:x} \n",
    "        m = hddm.HDDMRegressor(mydata, v_reg,\n",
    "            include=['z', 'sv'], group_only_nodes=['sv'],\n",
    "            group_only_regressors=False, keep_regressor_trace=False, p_outlier=0.05)\n",
    "    elif model_name == 'regress_z_resp': #only prevresp\n",
    "        z_reg = {'model': 'z ~ 1  + ' + resps, 'link_func': z_link_func}\n",
    "        v_reg = {'model': 'v ~ 1 + direction', 'link_func': lambda x:x}\n",
    "        m = hddm.HDDMRegressor(mydata, [z_reg, v_reg],\n",
    "            include=['z', 'sv'], group_only_nodes=['sv'],\n",
    "            group_only_regressors=False, keep_regressor_trace=False, p_outlier=0.05)\n",
    "    elif model_name == 'regress_dcz_resp': #only prevresp\n",
    "        v_reg = {'model': 'v ~ 1 + direction + ' + resps, 'link_func': lambda x:x}\n",
    "        z_reg = {'model': 'z ~ 1  + ' + resps, 'link_func': z_link_func}\n",
    "        m = hddm.HDDMRegressor(mydata, [v_reg, z_reg],\n",
    "                               include=['z', 'sv'], group_only_nodes=['sv'],\n",
    "                               group_only_regressors=False, keep_regressor_trace=False, p_outlier=0.05)\n",
    "    elif model_name == 'regress_dc_resp_stim': #both prevresp and prevstim\n",
    "        v_reg = {'model': 'v ~ 1 + direction + ' + respstim, 'link_func': lambda x:x}\n",
    "        m = hddm.HDDMRegressor(mydata, v_reg,\n",
    "            include=['z', 'sv'], group_only_nodes=['sv'],\n",
    "            group_only_regressors=False, keep_regressor_trace=False, p_outlier=0.05)\n",
    "    elif model_name == 'regress_z_resp_stim':\n",
    "        z_reg = {'model': 'z ~ 1  + ' + respstim, 'link_func': z_link_func}\n",
    "        v_reg = {'model': 'v ~ 1 + direction', 'link_func': lambda x:x}\n",
    "        m = hddm.HDDMRegressor(mydata, [z_reg, v_reg],\n",
    "            include=['z', 'sv'], group_only_nodes=['sv'],\n",
    "            group_only_regressors=False, keep_regressor_trace=False, p_outlier=0.05)\n",
    "    elif model_name == 'regress_dcz_resp_stim':\n",
    "        v_reg = {'model': 'v ~ 1 + direction + ' + respstim, 'link_func': lambda x:x}\n",
    "        z_reg = {'model': 'z ~ 1  + '+ respstim, 'link_func': z_link_func}\n",
    "        m = hddm.HDDMRegressor(mydata, [v_reg, z_reg],\n",
    "                               include=['z', 'sv'], group_only_nodes=['sv'],\n",
    "                               group_only_regressors=False, keep_regressor_trace=False, p_outlier=0.05)\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8cbc2665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_model_name(m,lag):\n",
    "    return m if 'nohist' in m else m + '_l' + str(lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "632849b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(m, mypath, model_name, trace_id, n_samples):\n",
    "    print(\"Running {:<s}, trace_id {:2d}\".format(model_name,trace_id))\n",
    "    print(\"finding starting values\")\n",
    "    try:\n",
    "        m.find_starting_values() # this should help the sampling\n",
    "    except Exception as e:\n",
    "        print(e) #even if starting values couldnt be found, sampling can continue\n",
    "\n",
    "    print(\"begin sampling\")\n",
    "    #m.sample(n_samples, burn=n_samples/2, thin=3)\n",
    "    \n",
    "    m.sample(n_samples, burn=n_samples/2, thin=3, db='pickle',\n",
    "        dbname=os.path.join(mypath, model_name, 'modelfit-md%d.db'%trace_id))\n",
    "    m.save(os.path.join(mypath, model_name, 'modelfit-md%d.model'%trace_id)) # save the model to disk\n",
    "    \n",
    "    # ============================================ #\n",
    "    # save the output values\n",
    "    # ============================================ #\n",
    "\n",
    "    # save the DIC for this model\n",
    "    text_file = open(os.path.join(mypath, model_name, 'DIC-md%d.txt'%trace_id), 'w')\n",
    "    text_file.write(\"Model {}: {}\\n\".format(trace_id, m.dic))\n",
    "    text_file.close()\n",
    "\n",
    "    # save the other model comparison indices\n",
    "    df = dict()\n",
    "    df['trace_id'] = trace_id\n",
    "    df['dic_original'] = [m.dic]\n",
    "    df['aic'] = [aic(m)]\n",
    "    df['bic'] = [bic(m)]\n",
    "    df2 = pd.DataFrame(df)\n",
    "    df2.to_csv(os.path.join(mypath, model_name, 'model_comparison_md%d.csv'%trace_id),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "681c9d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The set of models that can be built currently\n",
    "\n",
    "models_collection = {\n",
    "    1: 'stimcoding_nohist',  # no history baseline model\n",
    "    2: 'stimcoding_nohist_svgroup', # no history baseline model\n",
    "    3: 'stimcoding_dc_resp',  #previous response dependent, nlags needed\n",
    "    4: 'stimcoding_z_resp',  #previous response dependent, nlags needed\n",
    "    5: 'stimcoding_dc_z_resp', #previous response dependent, nlags needed\n",
    "    6: 'stimcoding_dc_stim',  #previous stimulus dependent, nlags needed\n",
    "    7: 'stimcoding_z_stim',  #previous stimulus dependent, nlags needed\n",
    "    8: 'stimcoding_dc_z_stim',  #previous stimulus dependent, nlags needed\n",
    "    9: 'stimcoding_dc_z_st_resp',  #previous response dependent, nlags needed\n",
    "    10:'stimcoding_dc_z_resp_svgroup',  #previous response dependent, nlags needed\n",
    "    11:'stimcoding_dc_z_resp_groupsplit',  #previous response dependent, nlags needed\n",
    "    12:'stimcoding_dc_z_resp_congruency',  #previous response dependent, nlags needed\n",
    "    13:'regress_dc_resp',  #only previous response factored, nlags needed\n",
    "    14:'regress_z_resp',  #only previous response factored, nlags needed\n",
    "    15:'regress_dcz_resp',  #only previous response factored, nlags needed\n",
    "    16:'regress_dc_resp_stim',  #both previous response fand stimulus actored, nlags needed\n",
    "    17:'regress_z_resp_stim',  #both previous response fand stimulus actored, nlags needed\n",
    "    18:'regress_dcz_resp_stim'  #both previous response fand stimulus actored, nlags needed\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c4420da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "prepare for running.\n",
    "Setup the folders, select model and the nth prev trial to be analyzed, fetch data\n",
    "\"\"\"\n",
    "mypath = \"output\"\n",
    "datasrc = \"~/Documents/Techspace/ddmown/data/coded/pair55.csv\"\n",
    "model = models_collection[8]\n",
    "lag = 3\n",
    "chains = 2\n",
    "trace_ids = range(1,chains+1)\n",
    "n_samples = 50\n",
    "\n",
    "full_model_name = get_full_model_name(model,lag)\n",
    "thispath = os.path.join(mypath,full_model_name)\n",
    "if not os.path.exists(thispath):\n",
    "    os.makedirs(thispath)\n",
    "\n",
    "mydata = hddm.load_csv(datasrc)\n",
    "mydata.rename(columns={'subject_id':'subj_idx'},inplace=True) #https://groups.google.com/g/hddm-users/c/Dhohjq_U2kU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ceabefbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running stimcoding_dc_z_stim_l3, trace_id  1\n",
      "finding starting values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sabi/Documents/Techspace/mambaforge/envs/dyadic/lib/python3.5/site-packages/scipy/optimize/optimize.py:1927: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  tmp2 = (x - v) * (fx - fw)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin sampling\n",
      " [-----------------100%-----------------] 50 of 50 complete in 1.1 sec\n",
      "Elapsed time for stimcoding_dc_z_stim, trace_id 1, 50 samples: 2.541885 seconds\n",
      "\n",
      "Running stimcoding_dc_z_stim_l3, trace_id  2\n",
      "finding starting values\n",
      "begin sampling\n",
      " [-----------------100%-----------------] 50 of 50 complete in 0.9 sec\n",
      "Elapsed time for stimcoding_dc_z_stim, trace_id 2, 50 samples: 5.154841 seconds\n",
      "\n",
      "Combining all traces for stimcoding_dc_z_stim_l3\n",
      "output/stimcoding_dc_z_stim_l3/modelfit-md1.model\n",
      "output/stimcoding_dc_z_stim_l3/modelfit-md2.model\n",
      "Performing gelman rubin convergence test\n",
      "\n",
      "non-convergence found, v_subj.55_2:0.935507447034\n",
      "non-convergence found, t_subj.55_1:1.27860136891\n",
      "non-convergence found, t_subj.55_2:1.35502893839\n",
      "non-convergence found, z_std:1.9303696259\n",
      "non-convergence found, a_std:0.935822796858\n",
      "non-convergence found, a_subj.55_1:1.12434987822\n",
      "non-convergence found, t:1.17362128484\n",
      "non-convergence found, t_std:1.20494350185\n",
      "non-convergence found, a:0.951280307384\n",
      "non-convergence found, v_subj.55_1:1.05076161102\n",
      "non-convergence found, dc_subj(55_1).55_2:1.57734026899\n",
      "non-convergence found, v_std:1.41496584034\n",
      "non-convergence found, z_subj_trans(55_1).55_2:1.17835177037\n",
      "non-convergence found, dc(55_1):1.64961709851\n",
      "non-convergence found, z_subj_trans(55_1).55_1:1.8602122591\n",
      "non-convergence found, z_subj_trans(55_2).55_1:1.26035559566\n",
      "non-convergence found, z_trans(55_1):2.29683082167\n",
      "non-convergence found, z_trans(55_2):1.3202129649\n",
      "non-convergence found, sz:1.47036341332\n",
      "non-convergence found, dc_subj(55_2).55_2:0.979356412125\n",
      "non-convergence found, sv:0.96653049705\n",
      "non-convergence found, dc_subj(55_1).55_1:1.51270771129\n",
      "non-convergence found, z_subj_trans(55_2).55_2:1.53586659851\n",
      "written gelman rubin stats to file\n",
      "Concatenated model saved!\n",
      "saving stats...\n",
      "done\n",
      "saving traces...\n",
      "done\n",
      "concatenating model comparison...\n",
      "done\n",
      "Now deleting files for seperate chains...\n",
      "output/stimcoding_dc_z_stim_l3/modelfit-md2.model\n",
      "output/stimcoding_dc_z_stim_l3/modelfit-md1.model\n",
      "output/stimcoding_dc_z_stim_l3/modelfit-md2.db\n",
      "output/stimcoding_dc_z_stim_l3/model_comparison_md1.csv\n",
      "output/stimcoding_dc_z_stim_l3/model_comparison_md2.csv\n",
      "output/stimcoding_dc_z_stim_l3/DIC-md2.txt\n",
      "output/stimcoding_dc_z_stim_l3/DIC-md1.txt\n",
      "DONE!!!\n"
     ]
    }
   ],
   "source": [
    "# ============================================ #\n",
    "# Main HDDM parameter estimation\n",
    "# ============================================ #\n",
    "starttime = time.time()\n",
    "for trace_id in trace_ids:\n",
    "    model_filename = os.path.join(mypath, full_model_name, 'modelfit-md%d.model' % trace_id)\n",
    "    m = make_model(mydata, model, trace_id, lag)\n",
    "    run_model(m, mypath, full_model_name, trace_id, n_samples)\n",
    "    elapsed = time.time() - starttime\n",
    "    print(\"\\nElapsed time for %s, trace_id %d, %d samples: %f seconds\\n\" % (model, trace_id, n_samples, elapsed))\n",
    "\n",
    "concat_models(mypath, full_model_name, chains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5c239f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing ppc\n",
      " [------------------------137%------------------------] 11 of 8 complete in 41.0 sec\n",
      "Elapsed time for stimcoding_dc_z_stim_l3, PPC: 42.566101 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================ #\n",
    "# POSTERIOR PREDICTIVES TO ASSESS MODEL FIT\n",
    "# ============================================ #\n",
    "\n",
    "starttime = time.time()\n",
    "print(\"computing ppc\")\n",
    "# specify how many samples are needed\n",
    "m = hddm.load(os.path.join(mypath,full_model_name, 'modelfit-combined.model'))\n",
    "nsmp = 100\n",
    "ppc = hddm.utils.post_pred_gen(m, append_data=True, samples=nsmp)\n",
    "\n",
    "# make the csv smaller, save disk space\n",
    "savecols = list(set(ppc.columns) & set(['rt','rt_sampled', 'response_sampled',\n",
    "                        'index', 'direction', 'response', 'l1_resp', 'subj_idx']))\n",
    "ppc = ppc[savecols]\n",
    "# save as pandas dataframe\n",
    "ppc.to_csv(os.path.join(mypath, full_model_name, 'ppc_data.csv'), index=True)\n",
    "elapsed = time.time() - starttime\n",
    "print( \"\\nElapsed time for %s, PPC: %f seconds\\n\" %(full_model_name,elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3d6943fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sabi/Documents/Techspace/mambaforge/envs/dyadic/lib/python3.5/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "/Users/sabi/Documents/Techspace/mambaforge/envs/dyadic/lib/python3.5/site-packages/scipy/optimize/optimize.py:1927: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  tmp2 = (x - v) * (fx - fw)\n",
      "/Users/sabi/Documents/Techspace/mambaforge/envs/dyadic/lib/python3.5/site-packages/scipy/optimize/optimize.py:1928: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  p = (x - v) * tmp2 - (x - w) * tmp1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 514.647160\n",
      "         Iterations: 15\n",
      "         Function evaluations: 1737\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 514.670952\n",
      "         Iterations: 16\n",
      "         Function evaluations: 1861\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 538.668771\n",
      "         Iterations: 10\n",
      "         Function evaluations: 2048\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 515.322621\n",
      "         Iterations: 8\n",
      "         Function evaluations: 857\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 515.037819\n",
      "         Iterations: 16\n",
      "         Function evaluations: 1885\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 510.077365\n",
      "         Iterations: 11\n",
      "         Function evaluations: 1318\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 509.522592\n",
      "         Iterations: 13\n",
      "         Function evaluations: 1527\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 510.254829\n",
      "         Iterations: 10\n",
      "         Function evaluations: 1249\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 510.913629\n",
      "         Iterations: 11\n",
      "         Function evaluations: 1514\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 508.788029\n",
      "         Iterations: 17\n",
      "         Function evaluations: 2125\n",
      "QUANTILE OPTIMISATION. DONE!!!\n"
     ]
    }
   ],
   "source": [
    "# ============================================ #\n",
    "# QUANTILE OPTIMISATION\n",
    "# http://ski.clps.brown.edu/hddm_docs/howto.html#run-quantile-opimization\n",
    "# ============================================ #\n",
    "\n",
    "subj_params = []\n",
    "bic = []\n",
    "\n",
    "for subj_idx, subj_data in mydata.groupby('subj_idx'):\n",
    "    m_subj = make_model(subj_data, model, 1,lag)\n",
    "    thismodel = m_subj.optimize('gsquare', quantiles=[0.1, 0.3, 0.5, 0.7, 0.9], n_runs=5)\n",
    "    thismodel.update({'subj_idx':subj_idx}) # keep original subject number\n",
    "    subj_params.append(thismodel)\n",
    "    bic.append(m_subj.bic_info)\n",
    "\n",
    "params = pd.DataFrame(subj_params)\n",
    "params.to_csv(os.path.join(mypath, full_model_name, 'Gsquare.csv'))\n",
    "bic = pd.DataFrame(bic)\n",
    "bic.to_csv(os.path.join(mypath, full_model_name, 'BIC.csv'))\n",
    "print(\"QUANTILE OPTIMISATION. DONE!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca2db0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
